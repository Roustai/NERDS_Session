{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainData(Dataset):\n",
    "\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(12, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0        Date  AveragePrice  Total Volume     4046       4225  \\\n",
      "0           0  2015-12-27          1.33      64236.62  1036.74   54454.85   \n",
      "1           1  2015-12-20          1.35      54876.98   674.28   44638.81   \n",
      "2           2  2015-12-13          0.93     118220.22   794.70  109149.67   \n",
      "3           3  2015-12-06          1.08      78992.15  1132.00   71976.41   \n",
      "4           4  2015-11-29          1.28      51039.60   941.48   43838.39   \n",
      "\n",
      "     4770  Total Bags  Small Bags  Large Bags  XLarge Bags          type  \\\n",
      "0   48.16     8696.87     8603.62       93.25          0.0  conventional   \n",
      "1   58.33     9505.56     9408.07       97.49          0.0  conventional   \n",
      "2  130.50     8145.35     8042.21      103.14          0.0  conventional   \n",
      "3   72.58     5811.16     5677.40      133.76          0.0  conventional   \n",
      "4   75.78     6183.95     5986.26      197.69          0.0  conventional   \n",
      "\n",
      "   year  region  \n",
      "0  2015  Albany  \n",
      "1  2015  Albany  \n",
      "2  2015  Albany  \n",
      "3  2015  Albany  \n",
      "4  2015  Albany  \n"
     ]
    }
   ],
   "source": [
    "#read in files\n",
    "file_in = os.getcwd() + \"/avacados/avocado.csv\"\n",
    "df = pd.read_csv(file_in)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.head)\n",
    "X = df.iloc[:, 0:-1] #grab first 13 elements for input\n",
    "y = df.iloc[:, -1]   #seperate last element (target values)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape data into a tensor of floats\n",
    "train_data = trainData(torch.FloatTensor(X_train),torch.FloatTensor(y))\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=5, shuffle=True)\n",
    "\n",
    "#build model and pass it to device (CPU or GPU)\n",
    "model = NeuralNetwork().to(device)\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_actual):\n",
    "    #print(y_pred, y_actual)\n",
    "    diff = (abs(y_pred - y_actual)/(y_pred + y_actual)).mean()\n",
    "    diff = (1 - diff) * 100\n",
    "    #print(diff)\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch 000: | Loss: 3.56090 | Acc: 99.185\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch 001: | Loss: 3.71723 | Acc: 99.449\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch 002: | Loss: 3.81223 | Acc: 99.568\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch 003: | Loss: 3.33987 | Acc: 98.729\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch 004: | Loss: 3.46746 | Acc: 99.693\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch 005: | Loss: 3.49470 | Acc: 99.317\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch 006: | Loss: 3.52524 | Acc: 99.260\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch 007: | Loss: 3.60855 | Acc: 99.370\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch 008: | Loss: 3.21427 | Acc: 99.840\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch 009: | Loss: 3.35218 | Acc: 99.383\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch 010: | Loss: 3.90428 | Acc: 98.692\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch 011: | Loss: 3.40690 | Acc: 99.546\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch 012: | Loss: 3.90770 | Acc: 99.704\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch 013: | Loss: 3.58448 | Acc: 99.572\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch 014: | Loss: 3.54929 | Acc: 99.416\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch 015: | Loss: 3.35499 | Acc: 99.343\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch 016: | Loss: 3.89931 | Acc: 99.125\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch 017: | Loss: 3.58070 | Acc: 97.382\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch 018: | Loss: 3.46805 | Acc: 99.375\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch 019: | Loss: 3.60300 | Acc: 99.583\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch 020: | Loss: 3.25254 | Acc: 99.384\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch 021: | Loss: 3.41902 | Acc: 99.695\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch 022: | Loss: 3.24037 | Acc: 99.444\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch 023: | Loss: 3.29186 | Acc: 99.352\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch 024: | Loss: 3.32901 | Acc: 99.197\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch 025: | Loss: 3.83716 | Acc: 99.695\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch 026: | Loss: 3.23147 | Acc: 99.446\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch 027: | Loss: 3.54272 | Acc: 99.387\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch 028: | Loss: 3.42649 | Acc: 99.203\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch 029: | Loss: 3.58386 | Acc: 98.182\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Epoch 030: | Loss: 3.48788 | Acc: 99.631\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Epoch 031: | Loss: 3.46781 | Acc: 99.499\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Epoch 032: | Loss: 3.58628 | Acc: 98.865\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Epoch 033: | Loss: 3.37192 | Acc: 99.233\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Epoch 034: | Loss: 3.63971 | Acc: 99.494\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Epoch 035: | Loss: 3.37947 | Acc: 99.389\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Epoch 036: | Loss: 3.44768 | Acc: 99.598\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Epoch 037: | Loss: 3.59474 | Acc: 99.022\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Epoch 038: | Loss: 3.44388 | Acc: 99.510\n",
      "Epoch 40\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a7702f13c5af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    print(f\"Epoch {e + 1}\\n-------------------------------\")\n",
    "    for features, labels in train_loader:\n",
    "\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        y_pred = model(features)\n",
    "        loss = loss_fn(y_pred, labels.unsqueeze(1))\n",
    "        acc = accuracy(y_pred, labels.unsqueeze(1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    print(f'Epoch {e + 0:03}: | Loss: {epoch_loss / len(train_loader):.5f} | Acc: {acc:.3f}')\n",
    "\n",
    "print(\"Done Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
